{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bertfine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1LBwNNpbrQWx9wE6hLX5cjBhl37Di4t3X",
      "authorship_tag": "ABX9TyOHOuzxgrXU+vQNEwboKAJu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SidNimbalkar/CSYE7245FinalProject/blob/master/Bertfine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8prXFBHskqs_",
        "colab_type": "code",
        "outputId": "099cfd92-cb22-4a56-c368-68d1aace9a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll4x7dUBk1Uq",
        "colab_type": "code",
        "outputId": "f5ca01d7-2c73-400b-9911-af06bdf0288b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhBddZs8k4nD",
        "colab_type": "code",
        "outputId": "602546b1-6459-4893-993b-bf6adfb6d9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7PJi7Rik83e",
        "colab_type": "code",
        "outputId": "c6d6ac9c-8af8-4875-a06b-29bdd074aff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"train.tsv\", delimiter='\\t', header=None, names=['label','tweet'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 1,041,119\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>351557</th>\n",
              "      <td>1</td>\n",
              "      <td>ruhut please deh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855030</th>\n",
              "      <td>2</td>\n",
              "      <td>back home tired tan and smiling ha ha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837978</th>\n",
              "      <td>2</td>\n",
              "      <td>gave us enough time to have a few beers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42130</th>\n",
              "      <td>1</td>\n",
              "      <td>one day old baby dies of coronavirus complicat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185117</th>\n",
              "      <td>0</td>\n",
              "      <td>boo should have told me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387656</th>\n",
              "      <td>1</td>\n",
              "      <td>the start of a new day at work</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652536</th>\n",
              "      <td>1</td>\n",
              "      <td>i got sin i jus got bought pairs of shoes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459344</th>\n",
              "      <td>2</td>\n",
              "      <td>but i do still like her and want to see her in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441701</th>\n",
              "      <td>0</td>\n",
              "      <td>haven t wanted to commit so still haven t boug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>819913</th>\n",
              "      <td>2</td>\n",
              "      <td>not to worry you won t be out the back on your...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        label                                              tweet\n",
              "351557      1                                   ruhut please deh\n",
              "855030      2              back home tired tan and smiling ha ha\n",
              "837978      2            gave us enough time to have a few beers\n",
              "42130       1  one day old baby dies of coronavirus complicat...\n",
              "185117      0                            boo should have told me\n",
              "387656      1                     the start of a new day at work\n",
              "652536      1          i got sin i jus got bought pairs of shoes\n",
              "459344      2  but i do still like her and want to see her in...\n",
              "441701      0  haven t wanted to commit so still haven t boug...\n",
              "819913      2  not to worry you won t be out the back on your..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0coW7LWBltzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = df.tweet.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lftu_Flql63F",
        "colab_type": "code",
        "outputId": "1663e614-9209-4abd-f523-5cc00001697c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWByRkI6mDwU",
        "colab_type": "code",
        "outputId": "e26b3a12-8165-4214-c039-064c7476b672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', tweets[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(tweets[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  road to recovery central arkansas veterans healthcare system discharged its first covid long term ventilator pa\n",
            "Tokenized:  ['road', 'to', 'recovery', 'central', 'arkansas', 'veterans', 'healthcare', 'system', 'discharged', 'its', 'first', 'co', '##vid', 'long', 'term', 'vent', '##ila', '##tor', 'pa']\n",
            "Token IDs:  [2346, 2000, 7233, 2430, 6751, 8244, 9871, 2291, 14374, 2049, 2034, 2522, 17258, 2146, 2744, 18834, 11733, 4263, 6643]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ5JKCEQmPlm",
        "colab_type": "code",
        "outputId": "13a4c4cc-3638-4233-fadc-708c270a839b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for tweet in tweets:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(tweet, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYUs96Pzmbn_",
        "colab_type": "code",
        "outputId": "3893d4ff-5c7f-4b88-d807-7cad14def504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for tweet in tweets:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        tweet,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', tweets[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  road to recovery central arkansas veterans healthcare system discharged its first covid long term ventilator pa\n",
            "Token IDs: tensor([  101,  2346,  2000,  7233,  2430,  6751,  8244,  9871,  2291, 14374,\n",
            "         2049,  2034,  2522, 17258,  2146,  2744, 18834, 11733,  4263,  6643,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnV4yqJGnUh2",
        "colab_type": "code",
        "outputId": "9b6e58da-3bc0-4b1b-b18a-781d4abada6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "937,007 training samples\n",
            "104,112 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8euK6cnxnd3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW8hq18qpWOy",
        "colab_type": "code",
        "outputId": "28e16378-a3e7-43a0-de3e-a29bdc913a04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY8LASx7pbnR",
        "colab_type": "code",
        "outputId": "19f640a1-9bd6-49f3-88a9-c054a9e06c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (3, 768)\n",
            "classifier.bias                                                 (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjXoxRYCpi0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT0ivC03ptRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeEDTWcMpvop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1NTvPuQpyut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCVPXekHp1No",
        "colab_type": "code",
        "outputId": "045783d9-a359-4ff0-8274-5cbba461842a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 5000 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch 5,000  of  29,282.    Elapsed: 0:32:09.\n",
            "  Batch 10,000  of  29,282.    Elapsed: 1:04:17.\n",
            "  Batch 15,000  of  29,282.    Elapsed: 1:36:25.\n",
            "  Batch 20,000  of  29,282.    Elapsed: 2:08:33.\n",
            "  Batch 25,000  of  29,282.    Elapsed: 2:40:40.\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epcoh took: 3:08:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:06:23\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch 5,000  of  29,282.    Elapsed: 0:32:09.\n",
            "  Batch 10,000  of  29,282.    Elapsed: 1:04:15.\n",
            "  Batch 15,000  of  29,282.    Elapsed: 1:36:22.\n",
            "  Batch 20,000  of  29,282.    Elapsed: 2:08:30.\n",
            "  Batch 25,000  of  29,282.    Elapsed: 2:40:38.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 3:08:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:06:23\n",
            "\n",
            "Training complete!\n",
            "Total training took 6:29:05 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYfzhdnvzDCw",
        "colab_type": "code",
        "outputId": "66264fd5-0955-4bb4-e158-a4f8c0748222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.18</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3:08:10</td>\n",
              "      <td>0:06:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.87</td>\n",
              "      <td>3:08:08</td>\n",
              "      <td>0:06:23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.18         0.44           0.86       3:08:10         0:06:23\n",
              "2               0.14         0.44           0.87       3:08:08         0:06:23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x5jOQ9KaziV",
        "colab_type": "code",
        "outputId": "0ce32bc7-4e17-4329-dfde-08a2cf4c38e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejPKICzuMLOi",
        "colab_type": "code",
        "outputId": "a9365c34-1dd6-49a7-f2c8-d81426881339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IGMn_PPMUnu",
        "colab_type": "code",
        "outputId": "65c02b25-eb02-4f75-a270-2446aa989ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 427964K\n",
            "-rw-r--r-- 1 root root      2K Apr 17 03:31 config.json\n",
            "-rw-r--r-- 1 root root 427722K Apr 17 03:31 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root      1K Apr 17 03:31 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1K Apr 17 03:31 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    227K Apr 17 03:31 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylnLunWzMaj2",
        "colab_type": "code",
        "outputId": "05e1b20c-2d64-498c-f281-72cd4cd7e5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 418M Apr 17 03:31 ./model_save/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgRqYEZ_MheB",
        "colab_type": "code",
        "outputId": "57e219b2-f356-437d-8e90-8de28937258b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVq0aUbElnwu",
        "colab_type": "text"
      },
      "source": [
        "Saving the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjiRE7hgXyC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "filename = 'model'\n",
        "outfile = open(filename,'wb')\n",
        "pickle.dump(model.module if hasattr(model, 'module') else model,outfile)\n",
        "outfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O8uv5uslvC8",
        "colab_type": "text"
      },
      "source": [
        "Testing the model with a small dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3PkGjXdO05c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv('testtweets.csv', header=None)\n",
        "\n",
        "\n",
        "test_df_bert = pd.DataFrame({'label':test_df[0],'text': test_df[1].replace(r'\\n', ' ', regex=True)\n",
        "})\n",
        "\n",
        "test_df_bert.head()\n",
        "\n",
        "test_df_bert.to_csv('test.tsv', sep='\\t', index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0ipi_vHM0du",
        "colab_type": "code",
        "outputId": "ea51b76d-adbe-4548-c5e9-3375b6a7f1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "dftest = pd.read_csv(\"test.tsv\", delimiter='\\t', header=None, names=['label','tweet'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test tweets: {:,}\\n'.format(dftest.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "tweetstest = dftest.tweet.values\n",
        "labelstest = dftest.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_idstest = []\n",
        "attention_maskstest = []\n",
        "\n",
        "# For every sentence...\n",
        "for tweet in tweetstest:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dicttest = tokenizer.encode_plus(\n",
        "                        tweet,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_idstest.append(encoded_dicttest['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_maskstest.append(encoded_dicttest['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_idstest = torch.cat(input_idstest, dim=0)\n",
        "attention_maskstest = torch.cat(attention_maskstest, dim=0)\n",
        "labelstest = torch.tensor(labelstest)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_sizetest = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_idstest, attention_maskstest, labelstest)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_sizetest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test tweets: 561\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7rZ9CA6SzhF",
        "colab_type": "code",
        "outputId": "441dcf38-dde4-402a-fb2f-2e628e915cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_idstest)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('DONE.')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 561 test sentences...\n",
            "DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnn7lMHpVaVL",
        "colab_type": "code",
        "outputId": "867aa350-cb4c-4806-95cb-58a1d7ff1435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch. \n",
        "\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xsrGnZpfK06",
        "colab_type": "code",
        "outputId": "5a1929c0-085b-48cc-978f-b4d5fd4dec31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvtGKpe4fNXG",
        "colab_type": "code",
        "outputId": "40ef18a6-8dc7-4a7b-eb20-fd05b12ecd69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total MCC: 0.958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22uuUJnZl3Jf",
        "colab_type": "text"
      },
      "source": [
        "This demonstrates that with a pre-trained BERT model we can quickly and effectively create a high quality model with minimal effort and training time using the pytorch interface, regardless of the specific NLP task you are interested in."
      ]
    }
  ]
}
